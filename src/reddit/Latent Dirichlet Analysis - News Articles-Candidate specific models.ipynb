{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kippy/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/kippy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kippy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kippy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lemm = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# nltk downloads\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#initialize tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "DATA_DIR = '../../data/reddit/Article_data_2019/'\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\"SANDERS\",\"DELANEY\",\"WARREN\",\"HARRIS\",\"GILLIBRAND\",\"O'ROURKE\",\"KLOBUCHAR\",\"BOOKER\",\n",
    "    \"BUTTIGIEG\",\"GABBARD\",\"YANG\",\"INSLEE\",\"HICKENLOOPER\",\"WILLIAMSON\",\"TULSI\",\"CASTRO\",\"BIDEN\", \"BERNIE\",\n",
    "    \"BETO\", \"ROURKE\", \"BENNETT\", \"BULLOCK\", \"BLASIO\", \"TIM RYAN\", \"GRAVEL\"]\n",
    "candidates = set([x.lower() for x in candidates])\n",
    "\n",
    "candidate_dict = {'klobuchar': 'klobuchar',\n",
    " 'bennett': 'bennett',\n",
    " 'booker': 'booker',\n",
    " 'warren': 'warren',\n",
    " 'castro': 'castro',\n",
    " 'williamson': 'williamson',\n",
    " 'gabbard': 'gabbard',\n",
    " 'bernie': 'sanders',\n",
    " \"o'rourke\": \"o_rourke\",\n",
    " 'bullock': 'bullock',\n",
    " 'tim ryan': 'tim_ryan',\n",
    " 'sanders': 'sanders',\n",
    " 'biden': 'biden',\n",
    " 'hickenlooper': 'hickenlooper',\n",
    " 'blasio': 'de_blasio',\n",
    " 'yang': 'yang',\n",
    " 'delaney': 'delaney',\n",
    " 'gillibrand': 'gillibrand',\n",
    " 'beto': 'o_rourke',\n",
    " 'harris': 'harris',\n",
    " 'inslee': 'inslee',\n",
    " 'rourke': 'o_rourke',\n",
    " 'gravel': 'gravel',\n",
    " 'tulsi': 'tulsi',\n",
    " 'buttigieg': 'buttigieg'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(DATA_DIR + 'reddit_2019_06_15_with_article_text.pkl')\n",
    "df = df.append(pd.read_pickle(DATA_DIR + 'reddit_2019jun16tojul1_articleurls_with_text.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack article data dictionary into columns\n",
    "df = pd.concat( [df.drop(['article_data'],axis=1), df['article_data'].progress_apply(pd.Series)]\n",
    "                      ,axis = 1)\n",
    "df.drop(0, axis = 1, inplace=True)\n",
    "\n",
    "\n",
    "def df_column_uniquify(df):\n",
    "    df_columns = df.columns\n",
    "    new_columns = []\n",
    "    for item in df_columns:\n",
    "        counter = 0\n",
    "        newitem = item\n",
    "        while newitem in new_columns:\n",
    "            counter += 1\n",
    "            newitem = \"{}_{}\".format(item, counter)\n",
    "        new_columns.append(newitem)\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "df = df_column_uniquify(df)\n",
    "df = df[(~df['title_1'].isnull()) & (~df['text'].isnull())] #remove nulls\n",
    "df = df.drop_duplicates(subset=['url']) #okay to drop duplicates since we have text for all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_valid_pos(x):\n",
    "    \"\"\"Converts the pos tag returned by the nltk.pos_tag function to a format accepted by wordNetLemmatizer\"\"\"\n",
    "    x = x[0].upper() # extract first character of the POS tag\n",
    "    \n",
    "    # define mapping for the tag to correct tag.\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "               \"N\": wordnet.NOUN,\n",
    "               \"R\": wordnet.ADV,\n",
    "               \"V\": wordnet.VERB}\n",
    "    \n",
    "    return tag_dict.get(x, wordnet.NOUN)\n",
    "\n",
    "def get_lemma(sentence):\n",
    "    \"\"\"Given a sentence, derives the lemmatized version of the sentence\"\"\"\n",
    "    pos_tagged_text = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    lemm_list = []\n",
    "\n",
    "    for (word, tag) in pos_tagged_text:\n",
    "        lemm_list.append(word_lemm.lemmatize(word, pos = convert_to_valid_pos(tag)))\n",
    "    \n",
    "    return lemm_list\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    \"\"\"Lemmatizes text, removes stopwords and short words from given text.\"\"\"\n",
    "    lemm_list = get_lemma(text)\n",
    "    \n",
    "    tokens = [i for i in lemm_list if i not in en_stopwords]\n",
    "    \n",
    "    tokens = [token.lower() for token in tokens if len(token) > 3]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_lemmas'] = df['title_1'].progress_map(prepare_text_for_lda)\n",
    "df['text_lemmas'] = df['text'].progress_map(prepare_text_for_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-21a7ba9487cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcandidate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintersect\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candidate_title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_lemmas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candidate_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_lemmas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def intersection(lemma): \n",
    "    lemma = [token.lower() for token in lemma]\n",
    "    intersect = candidates.intersection(lemma)\n",
    "    return {candidate_dict[x] for x in intersect}\n",
    "\n",
    "df['candidate_title'] = df['title_lemmas'].progress_map(intersection)\n",
    "df['candidate_text'] = df['text_lemmas'].progress_map(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {DATA_DIR + 'LDA/'}\n",
    "filename = DATA_DIR + 'LDA/df.pkl'\n",
    "df.to_pickle(filename)\n",
    "# df = pd.read_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(dataframe, field, directory, data_name):\n",
    "    dictionary = gensim.corpora.Dictionary(dataframe[field])\n",
    "    corpus = list(dataframe[field].progress_map(dictionary.doc2bow))\n",
    "    pickle.dump(corpus, open(directory + data_name + '_corpus.pkl', 'wb'))\n",
    "    dictionary.save(directory + data_name + '_dictionary.gensim')\n",
    "    \n",
    "\n",
    "!mkdir {DATA_DIR + 'LDA/candidates/'}\n",
    "for candidate in set(candidate_dict.values()):\n",
    "    drr = DATA_DIR + 'LDA/candidates/{}/'.format(candidate)\n",
    "    !mkdir {drr}\n",
    "    save_corpus(df[df.candidate_text.map(lambda x: candidate in x)], 'title_lemmas', drr, 'article_title_2019_candidate_{}'.format(candidate))\n",
    "    save_corpus(df[df.candidate_text.map(lambda x: candidate in x)], 'text_lemmas', drr, 'article_text_2019_candidate_{}'.format(candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-21 16:25:49,826 : INFO : loading Dictionary object from ../../data/reddit/Article_data_2019/LDA/candidates/buttigieg/article_text_2019_candidate_buttigieg_dictionary.gensim\n",
      "/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-07-21 16:25:49,851 : INFO : loaded ../../data/reddit/Article_data_2019/LDA/candidates/buttigieg/article_text_2019_candidate_buttigieg_dictionary.gensim\n",
      "2019-07-21 16:25:50,043 : INFO : using symmetric alpha at 0.1\n",
      "2019-07-21 16:25:50,044 : INFO : using symmetric eta at 0.1\n",
      "2019-07-21 16:25:50,052 : INFO : using serial LDA version on this node\n",
      "2019-07-21 16:25:50,101 : INFO : running online LDA training, 10 topics, 10 passes over the supplied corpus of 3241 documents, updating every 6000 documents, evaluating every ~0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2019-07-21 16:25:50,103 : INFO : training LDA model using 3 processes\n",
      "2019-07-21 16:25:50,125 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:25:50,346 : INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:03,249 : INFO : topic #2 (0.100): 0.016*\"president\" + 0.014*\"trump\" + 0.009*\"2020\" + 0.009*\"buttigieg\" + 0.007*\"former\" + 0.007*\"democratic\" + 0.006*\"would\" + 0.006*\"announce\" + 0.006*\"candidate\" + 0.006*\"mayor\"\n",
      "2019-07-21 16:26:03,250 : INFO : topic #7 (0.100): 0.009*\"trump\" + 0.009*\"president\" + 0.008*\"buttigieg\" + 0.006*\"would\" + 0.006*\"candidate\" + 0.006*\"campaign\" + 0.005*\"make\" + 0.005*\"democratic\" + 0.004*\"think\" + 0.004*\"people\"\n",
      "2019-07-21 16:26:03,252 : INFO : topic #1 (0.100): 0.009*\"trump\" + 0.008*\"biden\" + 0.007*\"would\" + 0.007*\"buttigieg\" + 0.007*\"president\" + 0.006*\"democratic\" + 0.006*\"voter\" + 0.005*\"sanders\" + 0.004*\"make\" + 0.004*\"candidate\"\n",
      "2019-07-21 16:26:03,253 : INFO : topic #6 (0.100): 0.014*\"sanders\" + 0.011*\"campaign\" + 0.011*\"candidate\" + 0.009*\"democratic\" + 0.006*\"biden\" + 0.006*\"buttigieg\" + 0.005*\"make\" + 0.005*\"president\" + 0.005*\"would\" + 0.005*\"debate\"\n",
      "2019-07-21 16:26:03,254 : INFO : topic #8 (0.100): 0.015*\"buttigieg\" + 0.006*\"president\" + 0.005*\"south\" + 0.005*\"make\" + 0.005*\"people\" + 0.005*\"trump\" + 0.005*\"think\" + 0.005*\"bend\" + 0.004*\"pete\" + 0.004*\"candidate\"\n",
      "2019-07-21 16:26:03,255 : INFO : topic diff=6.363080, rho=1.000000\n",
      "2019-07-21 16:26:03,256 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:03,459 : INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:09,621 : INFO : topic #9 (0.100): 0.017*\"buttigieg\" + 0.008*\"trump\" + 0.007*\"mayor\" + 0.007*\"biden\" + 0.007*\"president\" + 0.006*\"candidate\" + 0.006*\"democratic\" + 0.006*\"south\" + 0.005*\"state\" + 0.005*\"presidential\"\n",
      "2019-07-21 16:26:09,623 : INFO : topic #4 (0.100): 0.011*\"trump\" + 0.008*\"democratic\" + 0.006*\"candidate\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"democrats\" + 0.005*\"also\" + 0.005*\"make\" + 0.005*\"warren\" + 0.004*\"buttigieg\"\n",
      "2019-07-21 16:26:09,624 : INFO : topic #6 (0.100): 0.015*\"sanders\" + 0.012*\"campaign\" + 0.012*\"candidate\" + 0.009*\"democratic\" + 0.006*\"biden\" + 0.005*\"buttigieg\" + 0.005*\"debate\" + 0.005*\"make\" + 0.005*\"people\" + 0.004*\"would\"\n",
      "2019-07-21 16:26:09,625 : INFO : topic #8 (0.100): 0.020*\"buttigieg\" + 0.007*\"south\" + 0.006*\"people\" + 0.006*\"bend\" + 0.006*\"think\" + 0.005*\"pete\" + 0.005*\"make\" + 0.005*\"president\" + 0.005*\"like\" + 0.005*\"mayor\"\n",
      "2019-07-21 16:26:09,626 : INFO : topic #5 (0.100): 0.013*\"biden\" + 0.012*\"candidate\" + 0.012*\"trump\" + 0.008*\"poll\" + 0.006*\"democratic\" + 0.006*\"harris\" + 0.006*\"campaign\" + 0.006*\"president\" + 0.006*\"percent\" + 0.006*\"sanders\"\n",
      "2019-07-21 16:26:09,628 : INFO : topic diff=0.552482, rho=0.525552\n",
      "2019-07-21 16:26:09,629 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:09,810 : INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:14,616 : INFO : topic #5 (0.100): 0.015*\"biden\" + 0.013*\"candidate\" + 0.013*\"trump\" + 0.010*\"poll\" + 0.007*\"democratic\" + 0.007*\"harris\" + 0.007*\"percent\" + 0.007*\"president\" + 0.007*\"more\" + 0.006*\"sanders\"\n",
      "2019-07-21 16:26:14,617 : INFO : topic #2 (0.100): 0.018*\"president\" + 0.014*\"trump\" + 0.012*\"2020\" + 0.010*\"former\" + 0.010*\"announce\" + 0.008*\"getty\" + 0.007*\"would\" + 0.007*\"mayor\" + 0.006*\"democratic\" + 0.006*\"buttigieg\"\n",
      "2019-07-21 16:26:14,618 : INFO : topic #6 (0.100): 0.015*\"sanders\" + 0.013*\"campaign\" + 0.012*\"candidate\" + 0.009*\"democratic\" + 0.006*\"biden\" + 0.005*\"buttigieg\" + 0.005*\"people\" + 0.005*\"debate\" + 0.005*\"make\" + 0.005*\"warren\"\n",
      "2019-07-21 16:26:14,619 : INFO : topic #3 (0.100): 0.008*\"biden\" + 0.008*\"buttigieg\" + 0.008*\"sanders\" + 0.007*\"candidate\" + 0.007*\"democratic\" + 0.006*\"trump\" + 0.006*\"president\" + 0.005*\"people\" + 0.005*\"would\" + 0.004*\"campaign\"\n",
      "2019-07-21 16:26:14,620 : INFO : topic #1 (0.100): 0.012*\"trump\" + 0.009*\"president\" + 0.008*\"would\" + 0.006*\"impeachment\" + 0.006*\"biden\" + 0.006*\"buttigieg\" + 0.006*\"democratic\" + 0.005*\"mueller\" + 0.005*\"court\" + 0.005*\"voter\"\n",
      "2019-07-21 16:26:14,622 : INFO : topic diff=0.446058, rho=0.465217\n",
      "2019-07-21 16:26:14,623 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:14,809 : INFO : PROGRESS: pass 3, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:19,123 : INFO : topic #0 (0.100): 0.010*\"candidate\" + 0.008*\"would\" + 0.007*\"biden\" + 0.007*\"democratic\" + 0.007*\"warren\" + 0.006*\"sanders\" + 0.006*\"buttigieg\" + 0.005*\"percent\" + 0.005*\"support\" + 0.005*\"campaign\"\n",
      "2019-07-21 16:26:19,125 : INFO : topic #3 (0.100): 0.008*\"buttigieg\" + 0.008*\"biden\" + 0.007*\"sanders\" + 0.007*\"candidate\" + 0.006*\"democratic\" + 0.006*\"trump\" + 0.005*\"president\" + 0.005*\"people\" + 0.005*\"would\" + 0.004*\"like\"\n",
      "2019-07-21 16:26:19,126 : INFO : topic #7 (0.100): 0.013*\"trump\" + 0.010*\"president\" + 0.008*\"buttigieg\" + 0.006*\"would\" + 0.005*\"campaign\" + 0.005*\"make\" + 0.005*\"think\" + 0.004*\"people\" + 0.004*\"candidate\" + 0.004*\"house\"\n",
      "2019-07-21 16:26:19,127 : INFO : topic #4 (0.100): 0.011*\"trump\" + 0.007*\"democratic\" + 0.006*\"people\" + 0.006*\"democrats\" + 0.005*\"like\" + 0.005*\"candidate\" + 0.005*\"also\" + 0.005*\"make\" + 0.005*\"house\" + 0.005*\"white\"\n",
      "2019-07-21 16:26:19,128 : INFO : topic #1 (0.100): 0.013*\"trump\" + 0.010*\"president\" + 0.008*\"would\" + 0.007*\"impeachment\" + 0.007*\"mueller\" + 0.006*\"court\" + 0.005*\"buttigieg\" + 0.005*\"democratic\" + 0.005*\"house\" + 0.005*\"democrats\"\n",
      "2019-07-21 16:26:19,130 : INFO : topic diff=0.357563, rho=0.421806\n",
      "2019-07-21 16:26:19,131 : INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:19,319 : INFO : PROGRESS: pass 4, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:23,351 : INFO : topic #9 (0.100): 0.026*\"buttigieg\" + 0.010*\"mayor\" + 0.008*\"south\" + 0.007*\"bend\" + 0.007*\"pete\" + 0.006*\"democratic\" + 0.006*\"trump\" + 0.006*\"president\" + 0.006*\"candidate\" + 0.006*\"presidential\"\n",
      "2019-07-21 16:26:23,352 : INFO : topic #4 (0.100): 0.012*\"trump\" + 0.007*\"democratic\" + 0.006*\"people\" + 0.006*\"democrats\" + 0.005*\"like\" + 0.005*\"also\" + 0.005*\"news\" + 0.005*\"candidate\" + 0.005*\"make\" + 0.005*\"house\"\n",
      "2019-07-21 16:26:23,353 : INFO : topic #0 (0.100): 0.010*\"candidate\" + 0.008*\"would\" + 0.007*\"democratic\" + 0.007*\"biden\" + 0.007*\"warren\" + 0.006*\"sanders\" + 0.005*\"support\" + 0.005*\"plan\" + 0.005*\"buttigieg\" + 0.005*\"voter\"\n",
      "2019-07-21 16:26:23,354 : INFO : topic #5 (0.100): 0.019*\"biden\" + 0.014*\"trump\" + 0.013*\"candidate\" + 0.012*\"poll\" + 0.009*\"percent\" + 0.008*\"democratic\" + 0.008*\"more\" + 0.008*\"harris\" + 0.007*\"president\" + 0.007*\"sanders\"\n",
      "2019-07-21 16:26:23,355 : INFO : topic #7 (0.100): 0.014*\"trump\" + 0.010*\"president\" + 0.007*\"buttigieg\" + 0.006*\"would\" + 0.005*\"campaign\" + 0.004*\"think\" + 0.004*\"make\" + 0.004*\"people\" + 0.004*\"house\" + 0.004*\"time\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-21 16:26:23,357 : INFO : topic diff=0.287333, rho=0.388646\n",
      "2019-07-21 16:26:23,358 : INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:23,534 : INFO : PROGRESS: pass 5, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:27,434 : INFO : topic #4 (0.100): 0.012*\"trump\" + 0.007*\"democratic\" + 0.006*\"people\" + 0.006*\"democrats\" + 0.005*\"news\" + 0.005*\"like\" + 0.005*\"also\" + 0.005*\"abortion\" + 0.005*\"vote\" + 0.005*\"make\"\n",
      "2019-07-21 16:26:27,436 : INFO : topic #2 (0.100): 0.019*\"president\" + 0.013*\"2020\" + 0.012*\"trump\" + 0.012*\"announce\" + 0.011*\"former\" + 0.011*\"getty\" + 0.008*\"mayor\" + 0.007*\"would\" + 0.007*\"senator\" + 0.006*\"democratic\"\n",
      "2019-07-21 16:26:27,437 : INFO : topic #8 (0.100): 0.025*\"buttigieg\" + 0.008*\"people\" + 0.008*\"south\" + 0.008*\"bend\" + 0.007*\"think\" + 0.007*\"city\" + 0.007*\"pete\" + 0.007*\"mayor\" + 0.006*\"like\" + 0.005*\"make\"\n",
      "2019-07-21 16:26:27,438 : INFO : topic #1 (0.100): 0.015*\"trump\" + 0.011*\"president\" + 0.009*\"mueller\" + 0.008*\"impeachment\" + 0.008*\"would\" + 0.007*\"court\" + 0.006*\"house\" + 0.006*\"democrats\" + 0.006*\"report\" + 0.005*\"democratic\"\n",
      "2019-07-21 16:26:27,439 : INFO : topic #9 (0.100): 0.028*\"buttigieg\" + 0.011*\"mayor\" + 0.009*\"south\" + 0.007*\"bend\" + 0.007*\"pete\" + 0.007*\"democratic\" + 0.006*\"president\" + 0.006*\"black\" + 0.006*\"candidate\" + 0.006*\"presidential\"\n",
      "2019-07-21 16:26:27,441 : INFO : topic diff=0.232126, rho=0.362250\n",
      "2019-07-21 16:26:27,442 : INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:27,627 : INFO : PROGRESS: pass 6, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:31,341 : INFO : topic #0 (0.100): 0.010*\"candidate\" + 0.009*\"would\" + 0.007*\"democratic\" + 0.007*\"warren\" + 0.006*\"sanders\" + 0.006*\"biden\" + 0.005*\"plan\" + 0.005*\"support\" + 0.005*\"climate\" + 0.005*\"policy\"\n",
      "2019-07-21 16:26:31,342 : INFO : topic #1 (0.100): 0.015*\"trump\" + 0.011*\"president\" + 0.010*\"mueller\" + 0.009*\"impeachment\" + 0.008*\"would\" + 0.008*\"court\" + 0.006*\"house\" + 0.006*\"report\" + 0.006*\"democrats\" + 0.005*\"justice\"\n",
      "2019-07-21 16:26:31,343 : INFO : topic #2 (0.100): 0.020*\"president\" + 0.013*\"2020\" + 0.013*\"announce\" + 0.012*\"trump\" + 0.012*\"former\" + 0.012*\"getty\" + 0.008*\"mayor\" + 0.007*\"would\" + 0.007*\"senator\" + 0.006*\"democratic\"\n",
      "2019-07-21 16:26:31,344 : INFO : topic #4 (0.100): 0.012*\"trump\" + 0.007*\"democratic\" + 0.006*\"people\" + 0.006*\"news\" + 0.006*\"democrats\" + 0.005*\"like\" + 0.005*\"abortion\" + 0.005*\"also\" + 0.005*\"vote\" + 0.005*\"right\"\n",
      "2019-07-21 16:26:31,345 : INFO : topic #3 (0.100): 0.008*\"buttigieg\" + 0.007*\"biden\" + 0.007*\"sanders\" + 0.006*\"democratic\" + 0.006*\"candidate\" + 0.006*\"trump\" + 0.005*\"president\" + 0.005*\"make\" + 0.005*\"people\" + 0.004*\"like\"\n",
      "2019-07-21 16:26:31,346 : INFO : topic diff=0.188502, rho=0.340591\n",
      "2019-07-21 16:26:31,347 : INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:31,537 : INFO : PROGRESS: pass 7, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:35,206 : INFO : topic #7 (0.100): 0.016*\"trump\" + 0.011*\"president\" + 0.006*\"buttigieg\" + 0.006*\"would\" + 0.004*\"think\" + 0.004*\"make\" + 0.004*\"people\" + 0.004*\"house\" + 0.004*\"campaign\" + 0.004*\"u.s.\"\n",
      "2019-07-21 16:26:35,208 : INFO : topic #5 (0.100): 0.022*\"biden\" + 0.015*\"trump\" + 0.014*\"poll\" + 0.014*\"candidate\" + 0.011*\"percent\" + 0.010*\"democratic\" + 0.009*\"more\" + 0.008*\"harris\" + 0.008*\"democrats\" + 0.008*\"sanders\"\n",
      "2019-07-21 16:26:35,209 : INFO : topic #8 (0.100): 0.024*\"buttigieg\" + 0.009*\"people\" + 0.008*\"south\" + 0.008*\"think\" + 0.008*\"bend\" + 0.007*\"city\" + 0.007*\"like\" + 0.007*\"pete\" + 0.006*\"mayor\" + 0.005*\"make\"\n",
      "2019-07-21 16:26:35,210 : INFO : topic #4 (0.100): 0.012*\"trump\" + 0.007*\"democratic\" + 0.006*\"people\" + 0.006*\"news\" + 0.006*\"democrats\" + 0.005*\"like\" + 0.005*\"abortion\" + 0.005*\"vote\" + 0.005*\"right\" + 0.005*\"also\"\n",
      "2019-07-21 16:26:35,211 : INFO : topic #6 (0.100): 0.015*\"campaign\" + 0.015*\"sanders\" + 0.014*\"candidate\" + 0.009*\"democratic\" + 0.007*\"biden\" + 0.005*\"warren\" + 0.005*\"people\" + 0.005*\"buttigieg\" + 0.005*\"presidential\" + 0.005*\"bernie\"\n",
      "2019-07-21 16:26:35,212 : INFO : topic diff=0.153925, rho=0.322405\n",
      "2019-07-21 16:26:35,213 : INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:35,418 : INFO : PROGRESS: pass 8, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:39,013 : INFO : topic #7 (0.100): 0.017*\"trump\" + 0.011*\"president\" + 0.006*\"buttigieg\" + 0.006*\"would\" + 0.004*\"think\" + 0.004*\"make\" + 0.004*\"people\" + 0.004*\"house\" + 0.004*\"u.s.\" + 0.004*\"campaign\"\n",
      "2019-07-21 16:26:39,015 : INFO : topic #2 (0.100): 0.020*\"president\" + 0.013*\"announce\" + 0.013*\"2020\" + 0.012*\"former\" + 0.012*\"getty\" + 0.012*\"trump\" + 0.008*\"mayor\" + 0.007*\"senator\" + 0.007*\"would\" + 0.005*\"make\"\n",
      "2019-07-21 16:26:39,016 : INFO : topic #9 (0.100): 0.033*\"buttigieg\" + 0.012*\"mayor\" + 0.011*\"south\" + 0.009*\"bend\" + 0.008*\"pete\" + 0.007*\"black\" + 0.007*\"democratic\" + 0.006*\"president\" + 0.006*\"candidate\" + 0.006*\"presidential\"\n",
      "2019-07-21 16:26:39,017 : INFO : topic #3 (0.100): 0.008*\"buttigieg\" + 0.007*\"biden\" + 0.006*\"sanders\" + 0.006*\"democratic\" + 0.006*\"candidate\" + 0.006*\"trump\" + 0.005*\"president\" + 0.005*\"make\" + 0.004*\"like\" + 0.004*\"people\"\n",
      "2019-07-21 16:26:39,018 : INFO : topic #5 (0.100): 0.022*\"biden\" + 0.015*\"trump\" + 0.015*\"poll\" + 0.014*\"candidate\" + 0.011*\"percent\" + 0.010*\"democratic\" + 0.009*\"more\" + 0.009*\"democrats\" + 0.008*\"harris\" + 0.008*\"sanders\"\n",
      "2019-07-21 16:26:39,019 : INFO : topic diff=0.127121, rho=0.306851\n",
      "2019-07-21 16:26:39,020 : INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #2000/3241, outstanding queue size 1\n",
      "2019-07-21 16:26:39,207 : INFO : PROGRESS: pass 9, dispatched chunk #1 = documents up to #3241/3241, outstanding queue size 2\n",
      "2019-07-21 16:26:42,711 : INFO : topic #3 (0.100): 0.007*\"buttigieg\" + 0.007*\"biden\" + 0.006*\"sanders\" + 0.006*\"democratic\" + 0.006*\"candidate\" + 0.005*\"trump\" + 0.005*\"president\" + 0.005*\"make\" + 0.004*\"like\" + 0.004*\"people\"\n",
      "2019-07-21 16:26:42,712 : INFO : topic #4 (0.100): 0.012*\"trump\" + 0.007*\"democratic\" + 0.007*\"news\" + 0.006*\"people\" + 0.006*\"abortion\" + 0.005*\"like\" + 0.005*\"vote\" + 0.005*\"democrats\" + 0.005*\"right\" + 0.005*\"also\"\n",
      "2019-07-21 16:26:42,713 : INFO : topic #7 (0.100): 0.018*\"trump\" + 0.011*\"president\" + 0.006*\"would\" + 0.005*\"buttigieg\" + 0.004*\"think\" + 0.004*\"make\" + 0.004*\"house\" + 0.004*\"people\" + 0.004*\"u.s.\" + 0.004*\"campaign\"\n",
      "2019-07-21 16:26:42,715 : INFO : topic #1 (0.100): 0.017*\"trump\" + 0.012*\"president\" + 0.012*\"mueller\" + 0.009*\"impeachment\" + 0.009*\"court\" + 0.008*\"would\" + 0.007*\"report\" + 0.007*\"house\" + 0.006*\"democrats\" + 0.006*\"justice\"\n",
      "2019-07-21 16:26:42,716 : INFO : topic #9 (0.100): 0.035*\"buttigieg\" + 0.013*\"mayor\" + 0.011*\"south\" + 0.009*\"bend\" + 0.008*\"pete\" + 0.008*\"black\" + 0.007*\"democratic\" + 0.006*\"president\" + 0.006*\"presidential\" + 0.006*\"candidate\"\n",
      "2019-07-21 16:26:42,717 : INFO : topic diff=0.105953, rho=0.293351\n",
      "2019-07-21 16:26:42,964 : INFO : saving LdaState object under ../../data/reddit/Article_data_2019/LDA/candidates/buttigieg/models/10_model_10.gensim.state, separately None\n",
      "/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-07-21 16:26:42,985 : INFO : saved ../../data/reddit/Article_data_2019/LDA/candidates/buttigieg/models/10_model_10.gensim.state\n",
      "2019-07-21 16:26:43,007 : INFO : saving LdaMulticore object under ../../data/reddit/Article_data_2019/LDA/candidates/buttigieg/models/10_model_10.gensim, separately ['expElogbeta', 'sstats']\n",
      "2019-07-21 16:26:43,008 : INFO : storing np array 'expElogbeta' to ../../data/reddit/Article_data_2019/LDA/candidates/buttigieg/models/10_model_10.gensim.expElogbeta.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-21 16:26:43,015 : INFO : not storing attribute id2word\n",
      "2019-07-21 16:26:43,016 : INFO : not storing attribute dispatcher\n",
      "2019-07-21 16:26:43,016 : INFO : not storing attribute state\n",
      "/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-07-21 16:26:43,018 : INFO : saved ../../data/reddit/Article_data_2019/LDA/candidates/buttigieg/models/10_model_10.gensim\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "Topic_list = []\n",
    "num_topics = 10\n",
    "passes = 10\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "chunksize = 2000\n",
    "\n",
    "for candidate in set(candidate_dict.values()):\n",
    "    drr = DATA_DIR + 'LDA/candidates/{}/'.format(candidate)    \n",
    "    \n",
    "    dictionary = gensim.corpora.Dictionary.load(drr + 'article_text_2019_candidate_{}'.format(candidate) + '_dictionary.gensim')\n",
    "    corpus = pickle.load(open(drr + 'article_text_2019_candidate_{}'.format(candidate) + '_corpus.pkl', 'rb'))\n",
    "    # https://radimrehurek.com/gensim/models/ldamulticore.html\n",
    "    try:\n",
    "        ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus,\n",
    "                                                           num_topics = num_topics, \n",
    "                                                           id2word = dictionary, \n",
    "                                                           passes=passes,\n",
    "                                                           chunksize = chunksize,\n",
    "                                                           eval_every = eval_every,\n",
    "                                                           iterations = iterations,\n",
    "                                                           workers = 3) #set this to cores - 1\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    !mkdir {drr + 'models/'}\n",
    "    ldamodel.save(drr + 'models/' + '{}_model_{}.gensim'.format(num_topics, passes))\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "topics = ldamodel.print_topics(num_words = 20, num_topics=100)\n",
    "for topic in topics:\n",
    "    Topic_list.append(topic[1])\n",
    "# save the topics for later use.\n",
    "topic_df = pd.DataFrame({'topics':Topic_list})\n",
    "\n",
    "def clean_topic_words(x):\n",
    "    \"\"\"Clean topic words as output by the algorithm\"\"\"\n",
    "    clean_topic = re.findall(\"\\\".*?\\\"\", x)\n",
    "    clean_topic = [s.replace('\\\"', '') for s in clean_topic]\n",
    "    return clean_topic\n",
    "\n",
    "topic_df['topics'] = topic_df['topics'].map(clean_topic_words)\n",
    "\n",
    "!mkdir {DATA_DIR + 'LDA/topics'}\n",
    "topic_df.to_csv(DATA_DIR + \"/LDA/topics/Topics_List_{}_model_{}_{}.csv\".format(num_topics, passes, 'article_text_2019_candidate_only'),\n",
    "                index=False, header=False, \n",
    "                quoting=csv.QUOTE_NONE, sep = '\\n', escapechar='\\\\') # write out for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prepare() got an unexpected keyword argument 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7febfeae6b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: prepare() got an unexpected keyword argument 'sort'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary=dictionary, sort=True)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
